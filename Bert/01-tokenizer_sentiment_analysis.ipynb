{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70b09e04-97fd-461a-bce8-6d051f2b2c84",
   "metadata": {},
   "source": [
    "## 1. tokenizer, 构造输入\n",
    "- tokenizer, model: 相匹配, tokenizer outputs => model input\n",
    "- tokenizer\n",
    "    - tokenizer.encode = tokenizer.tokenize + tokenizer.convert_tokens_to_ids + `[CLS]` and `[SEP]`\n",
    "    - tokenizer.decode\n",
    "    - tokenizer 工作原理其实是tokenizer.vovab\n",
    "    - attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2892bde8-2304-4c2f-a419-6e06876989ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = ['today is not that bad', 'today is so bad', 'I am so happy']\n",
    "model_name = 'distilbert-base-uncased-finetuned-sst-2-english'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bcffd5b-3be5-4f21-80db-394e809d217e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3a119cc4-1810-4eb8-9427-7a36c65d149e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ffba696-c078-40ba-aa2a-9e6849d69300",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = tokenizer(test_sentences, truncation=True, max_length=256, padding=True, return_tensors='pt') # 一次性处理一个string list的话则需要设定后面的参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cafcc743-c03d-4bd6-b991-4a757503a81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2651, 2003, 2025, 2008, 2919,  102],\n",
       "        [ 101, 2651, 2003, 2061, 2919,  102,    0],\n",
       "        [ 101, 1045, 2572, 2061, 3407,  102,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "118b74f7-c69a-40d9-905b-50912d199d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101, 2651, 2003, 2025, 2008, 2919, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tokenizer, word to bert input. \n",
    "tokenizer(test_sentences[0])  # 一次性处理一个句子的话，则不需要添加参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc32f000-219f-427d-abd3-521326e1190b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 2651, 2003, 2025, 2008, 2919, 102]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# encode, word to index\n",
    "# Auto add [CLS] and [SEP]\n",
    "ids = tokenizer.encode(test_sentences[0]) \n",
    "ids "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e800f448-e8b8-4b8d-9ffa-76eb877df086",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['today', 'is', 'not', 'that', 'bad']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = tokenizer.tokenize(test_sentences[0]) \n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8dd4df1-a56c-499c-a750-9527af438884",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2651, 2003, 2025, 2008, 2919]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids(tokens) # without [CLS] and [SEP]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "163c4a4a-4c91-4f43-b112-f84521a3c616",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[CLS] today is not that bad [SEP]'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([101, 2651, 2003, 2025, 2008, 2919, 102])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c095afcc-a442-4f31-a3fa-c4d93e75ec8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[100, 102, 0, 101, 103]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.convert_tokens_to_ids([special for special in tokenizer.special_tokens_map.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99a67fcc-c81d-41bc-8445-da7e88d89a9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[ 101, 2651, 2003, 2025, 2008, 2919,  102],\n",
       "        [ 101, 2651, 2003, 2061, 2919,  102,    0],\n",
       "        [ 101, 1045, 2572, 2061, 3407,  102,    0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1],\n",
       "        [1, 1, 1, 1, 1, 1, 0],\n",
       "        [1, 1, 1, 1, 1, 1, 0]])}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer(test_sentences, max_length=32, truncation=True, padding='max_length', return_tensors='pt')\n",
    "tokenizer(test_sentences, truncation=True, padding=True, return_tensors='pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c4efa1-c09d-41ec-aaf8-cd15ab5f9b87",
   "metadata": {},
   "source": [
    "## 2、 模型构造"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "daa3890d-8b98-443b-b665-fb53c0334a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38f82e88-8992-4c2a-ae85-a5d600380b72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequenceClassifierOutput(loss=None, logits=tensor([[-3.4620,  3.6118],\n",
       "        [ 4.7508, -3.7899],\n",
       "        [-4.3472,  4.6912]], grad_fn=<AddmmBackward0>), hidden_states=None, attentions=None)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(**batch_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47c9069-f4a2-4a32-bc89-422071f82266",
   "metadata": {},
   "source": [
    "## 3、一个文本语料"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ac6515-f657-4a50-bedb-6e8e94cea090",
   "metadata": {},
   "source": [
    "- newsgroups_train.DESCR\n",
    "- newsgroups_train.data\n",
    "- newsgroups_train.target\n",
    "- newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b50cd68b-7ec3-40f3-8a31-6bc9dbc15abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "newsgroups_train = fetch_20newsgroups(subset='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2dfea4b9-ca75-474f-a38a-05fe1cc1837c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "00dabe4d-72c5-42b8-9375-17358dd34cce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11314"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(newsgroups_train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "30d24db3-211a-464a-9f08-0657103361b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({10: 600,\n",
       "         15: 599,\n",
       "         8: 598,\n",
       "         9: 597,\n",
       "         11: 595,\n",
       "         7: 594,\n",
       "         13: 594,\n",
       "         14: 593,\n",
       "         5: 593,\n",
       "         2: 591,\n",
       "         12: 591,\n",
       "         3: 590,\n",
       "         6: 585,\n",
       "         1: 584,\n",
       "         4: 578,\n",
       "         17: 564,\n",
       "         16: 546,\n",
       "         0: 480,\n",
       "         18: 465,\n",
       "         19: 377})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 统计\n",
    "from collections import Counter\n",
    "Counter(newsgroups_train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8004410f-720a-41b0-aed4-792e906646c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
